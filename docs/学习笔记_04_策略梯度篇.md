# 学习笔记 04 - 策略梯度篇

> 从 DQN 到 PPO：理解策略梯度方法的演进

---

## 核心概念

### Value-based vs Policy-based

| 类型 | 代表算法 | 核心思想 |
|------|----------|----------|
| **Value-based** | DQN 系列 | 学习 Q(s,a)，选择价值最高的动作 |
| **Policy-based** | REINFORCE, PPO | 直接学习策略 π(a|s)，输出动作概率 |

### 为什么需要策略梯度？

DQN 的局限：
1. 只能处理离散动作空间
2. 需要找最大值（max operation），计算复杂
3. 确定性策略可能丢失信息

策略梯度的优势：
1. 可以处理连续动作空间
2. 不需要 max operation
3. 可以输出随机策略（更适合探索）
4. 更自然的动作选择方式

---

## REINFORCE：蒙特卡洛策略梯度

### 算法原理

**目标**：最大化期望回报

**梯度公式**：
```
∇θ J(θ) = E[∇θ log π(a|s; θ) * G(t)]
```

**直观理解**：
- 如果某个动作获得了高回报 → 增加该动作的概率
- 如果获得了低回报 → 降低该动作的概率

### 伪代码

```
for each episode:
    for each step t:
        采样动作 a_t ~ π(·|s_t)
        执行动作，获得奖励 r_t
    # episode 结束后
    for each step t:
        计算回报 G(t) = r_t + γr_{t+1} + ...
        更新 θ = θ + α * ∇θ log π(a_t|s_t) * G(t)
```

### 优缺点

| 优点 | 缺点 |
|------|------|
| 理论简单 | 高方差（单次估计不稳定）|
| 无需价值函数 | 需要完整 episode（无法在线更新）|
| 收敛到局部最优 | 收敛慢 |

---

## Actor-Critic：引入评论家

### 核心思想

REINFORCE 的问题：用 G(t) 作为目标，方差太大

解决方案：用 Critic 估计状态价值 V(s)，作为基线

**Advantage 函数**：
```
A(s,a) = Q(s,a) - V(s) ≈ r + γV(s') - V(s)
```

直观理解：这个动作比"平均水平"好多少？

### 架构

```
Actor (演员)         Critic (评论家)
   状态                状态
    ↓                   ↓
  网络 → 动作概率      网络 → 价值 V(s)
    ↓
  采样动作
```

### A2C vs A3C

| 特性 | A2C | A3C |
|------|-----|-----|
| 更新方式 | 同步（多个 worker）| 异步 |
| 实现难度 | 简单 | 复杂 |
| 资源利用 | 更好 | 一般 |

实践中 A2C 更受欢迎（因为可以用批量并行）。

---

## PPO：稳定高效的策略梯度

### 问题背景

策略梯度有一个大问题：**更新过大可能崩溃**

想象：你在调参，一步走错，策略变差，再也无法恢复

### PPO 的解决方案

**Clipped Surrogate Objective**：

```
L_CLIP(θ) = E[min(r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A)]

其中 r(θ) = π_new(a|s) / π_old(a|s)
```

**直观理解**：
1. 计算新旧策略的概率比 r
2. 如果 r 太大（> 1+ε）：说明新策略太激进，裁剪它
3. 如果 r 太小（< 1-ε）：说明新策略太保守，也不惩罚太多

### 为什么 PPO 这么受欢迎？

1. **简单**：容易实现和调试
2. **稳定**：不容易崩溃
3. **高效**：一批数据可以多次使用
4. **通用**：适用于各种任务

### PPO 伪代码

```
for each iteration:
    # 收集数据
    for n steps:
        用当前策略 π_old 采样动作
        存储 (s, a, r, V(s), log π_old(a|s))

    # 计算 Advantage
    使用 GAE 计算 A(s,a)

    # 多次更新（这是 PPO 高效的原因！）
    for k epochs:
        for each mini-batch:
            计算比率 r = π_new(a|s) / π_old(a|s)
            计算 L = -min(r*A, clip(r,1-ε,1+ε)*A)
            更新网络
```

---

## 实现经验

### REINFORCE 实现要点

```python
# 关键：整个 episode 存储后一起计算
def compute_returns(self):
    returns = []
    R = 0
    for reward in reversed(self.rewards):
        R = reward + self.gamma * R
        returns.insert(0, R)
    return returns

# 归一化回报（稳定训练的关键）
returns = (returns - returns.mean()) / (returns.std() + 1e-8)
```

### A2C 实现要点

```python
# 关键：Advantage 的计算
def compute_advantages(self):
    advantages = []
    for t in range(len(rewards)):
        if done[t]:
            advantage = rewards[t] - values[t]
        else:
            advantage = rewards[t] + gamma * values[t+1] - values[t]
        advantages.append(advantage)
    return advantages
```

### PPO 实现要点

```python
# 关键：Clipped Objective
ratio = torch.exp(new_log_prob - old_log_prob)
surr1 = ratio * advantage
surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage
policy_loss = -torch.min(surr1, surr2).mean()

# 关键：更新旧策略
self.old_actor.load_state_dict(self.actor.state_dict())
```

---

## 算法对比

| 算法 | 样本效率 | 稳定性 | 实现难度 | 适用场景 |
|------|----------|--------|----------|----------|
| REINFORCE | 低 | 低 | 简单 | 学习理解 |
| A2C | 中 | 中 | 中等 | 一般任务 |
| PPO | 高 | 高 | 中等 | 大部分任务（推荐）|

---

## 调参经验

### 学习率
- REINFORCE: 1e-3
- A2C: 1e-3 ~ 3e-4
- PPO: 3e-4（需要较小学习率）

### 网络结构
- 简单任务（CartPole）：2 层 64/128 单元
- 复杂任务：3-4 层，256 单元以上

### PPO 特有参数
- `clip_epsilon`: 0.2（默认，一般不需要调）
- `gae_lambda`: 0.95（默认）
- `n_epochs`: 4（可调整，更多 epoch = 更高样本效率）

---

## 下一步学习

1. **连续动作空间**：SAC, TD3, DDPG
2. **分布式训练**：IMPALA, A3C
3. **基于 Transformer**：Decision Transformer

---

*最后更新：2026-02-10*
