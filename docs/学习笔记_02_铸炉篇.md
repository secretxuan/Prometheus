# Prometheus å­¦ä¹ ç¬”è®° #02 - é“¸ç‚‰ç¯‡

> ç‰ˆæœ¬ v0.1.0 - æ¨¡å—åŒ–æ¶æ„è®¾è®¡

## ğŸ“– é˜¶æ®µäºŒç›®æ ‡

è®¾è®¡ Prometheus çš„åŸºç¡€æ¶æ„ï¼Œè®©ä»£ç æ›´æ¨¡å—åŒ–ã€æ›´æ˜“æ‰©å±•ã€‚

---

## ğŸ—ï¸ ä»€ä¹ˆæ˜¯å¥½çš„æ¡†æ¶è®¾è®¡ï¼Ÿ

### åŸåˆ™ 1ï¼šå…³æ³¨ç‚¹åˆ†ç¦»

æŠŠä¸åŒèŒè´£çš„ä»£ç æ”¾åˆ°ä¸åŒçš„æ¨¡å—ä¸­ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  è§‚å¯Ÿ  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Agent    â”‚ â†â”€â”€â”€â†’ â”‚    Env      â”‚
â”‚   (æ™ºèƒ½ä½“)   â”‚        â”‚   (ç¯å¢ƒ)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†‘                       â†‘
      â”‚ åŒ…å«                  â”‚ è¢«
      â”‚                       â”‚ åŒ…è£…
      â†“                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Policy    â”‚          â”‚   Wrapper   â”‚
â”‚   (ç­–ç•¥)     â”‚          â”‚   (åŒ…è£…å™¨)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†‘
      â”‚ è¢«
      â”‚ ä½¿ç”¨
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Trainer   â”‚
â”‚   (è®­ç»ƒå™¨)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åŸåˆ™ 2ï¼šä¾èµ–å€’ç½®

ä¾èµ–æŠ½è±¡ï¼ˆæ¥å£ï¼‰ï¼Œè€Œä¸æ˜¯ä¾èµ–å…·ä½“å®ç°ã€‚

```python
# âŒ ä¸å¥½ï¼šä¾èµ–å…·ä½“å®ç°
def train(agent: DQNAgent):
    ...

# âœ… å¥½ï¼šä¾èµ–æŠ½è±¡æ¥å£
def train(agent: BaseAgent):
    ...
```

### åŸåˆ™ 3ï¼šå¼€é—­åŸåˆ™

å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­ã€‚

æ·»åŠ æ–°ç®—æ³•æ—¶ï¼Œåº”è¯¥ï¼š
- âœ… æ·»åŠ æ–°æ–‡ä»¶ï¼ˆå¦‚ `ppo.py`ï¼‰
- âŒ ä¸ä¿®æ”¹ç°æœ‰ä»£ç 

---

## ğŸ“¦ æ–°æ¡†æ¶æ¨¡å—è¯¦è§£

### 1. envs æ¨¡å— - ç¯å¢ƒæŠ½è±¡

**é—®é¢˜**ï¼šä¸åŒçš„ RL åº“æœ‰ä¸åŒæ¥å£
**è§£å†³**ï¼šç»Ÿä¸€çš„ç¯å¢ƒæŠ½è±¡

```python
from prometheus.envs import make_gym_env

# ç»Ÿä¸€çš„æ¥å£åˆ›å»ºç¯å¢ƒ
env = make_gym_env("CartPole-v1")
obs, info = env.reset()
obs, reward, terminated, truncated, info = env.step(action)
```

**æ ¸å¿ƒç±»**ï¼š
- `EnvSpec` - ç¯å¢ƒè§„æ ¼ï¼ˆè§‚å¯Ÿç©ºé—´ã€åŠ¨ä½œç©ºé—´ç­‰ï¼‰
- `EnvWrapper` - ç¯å¢ƒåŒ…è£…å™¨æŠ½è±¡
- `GymWrapper` - Gymnasium ç¯å¢ƒçš„åŒ…è£…å™¨

---

### 2. policies æ¨¡å— - ç­–ç•¥æŠ½è±¡

**ä»€ä¹ˆæ˜¯ç­–ç•¥ï¼Ÿ**
ç­–ç•¥ = æ ¹æ®çŠ¶æ€å†³å®šåšä»€ä¹ˆåŠ¨ä½œ

**æ ¸å¿ƒç±»**ï¼š
- `BasePolicy` - ç­–ç•¥æŠ½è±¡åŸºç±»
- `TorchPolicy` - PyTorch ç­–ç•¥åŸºç±»ï¼ˆå¤„ç†è®¾å¤‡ã€æ¨¡å¼åˆ‡æ¢ï¼‰
- `DQNPolicy` - DQN ç­–ç•¥å®ç°

```python
from prometheus.policies import DQNPolicy

policy = DQNPolicy(state_dim=4, action_dim=2)
action = policy.select_action(state, training=True)
```

---

### 3. agents æ¨¡å— - æ™ºèƒ½ä½“æŠ½è±¡

**ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼Ÿ**
æ™ºèƒ½ä½“ = ç­–ç•¥ + å­¦ä¹ ç®—æ³• + è®°å¿†

**æ ¸å¿ƒç±»**ï¼š
- `BaseAgent` - æ™ºèƒ½ä½“æŠ½è±¡åŸºç±»
- `DQNAgent` - DQN æ™ºèƒ½ä½“å®ç°

```python
from prometheus.agents import DQNAgent

agent = DQNAgent(state_dim=4, action_dim=2)
action = agent.act(state)
agent.remember(state, action, reward, next_state, done)
metrics = agent.learn()
```

---

### 4. trainers æ¨¡å— - è®­ç»ƒå™¨æŠ½è±¡

**ä»€ä¹ˆæ˜¯è®­ç»ƒå™¨ï¼Ÿ**
è®­ç»ƒå™¨ = ç®¡ç†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„"æ•™ç»ƒ"

**æ ¸å¿ƒç±»**ï¼š
- `BaseTrainer` - è®­ç»ƒå™¨æŠ½è±¡åŸºç±»
- `TrainerConfig` - è®­ç»ƒé…ç½®
- `Callback` - å›è°ƒå‡½æ•°åŸºç±»
- `DQNTrainer` - DQN è®­ç»ƒå™¨å®ç°

```python
from prometheus.trainers import DQNTrainer, TrainerConfig

config = TrainerConfig(
    max_episodes=1000,
    eval_interval=100,
    log_interval=10
)
trainer = DQNTrainer(config)
trainer.train(env, agent)
```

---

## ğŸ”§ Python è®¾è®¡æ¨¡å¼å­¦ä¹ 

### 1. æŠ½è±¡åŸºç±» (ABC)

å¼ºåˆ¶å­ç±»å®ç°ç‰¹å®šæ–¹æ³•ï¼š

```python
from abc import ABC, abstractmethod

class BaseAgent(ABC):
    @abstractmethod
    def act(self, state):
        pass  # å­ç±»å¿…é¡»å®ç°

# è¿™æ ·å†™ä¼šæŠ¥é”™ï¼š
# agent = BaseAgent()  # TypeError!

# å¿…é¡»å®ç°æ‰€æœ‰æŠ½è±¡æ–¹æ³•æ‰èƒ½å®ä¾‹åŒ–
class MyAgent(BaseAgent):
    def act(self, state):
        return 0  # OK
```

### 2. dataclass

ç®€åŒ–æ•°æ®ç±»å®šä¹‰ï¼š

```python
from dataclasses import dataclass

# âŒ æ—§æ–¹å¼ï¼šå†™å¾ˆå¤š __init__
class Config:
    def __init__(self, max_episodes: int, log_interval: int):
        self.max_episodes = max_episodes
        self.log_interval = log_interval

# âœ… æ–°æ–¹å¼ï¼šdataclass
@dataclass
class Config:
    max_episodes: int
    log_interval: int
```

### 3. Mixin æ··å…¥ç±»

ç»™ç±»æ·»åŠ å¯é€‰åŠŸèƒ½ï¼š

```python
class StatefulMixin:
    """æœ‰çŠ¶æ€çš„ç­–ç•¥æ··å…¥ç±»"""
    def reset_state(self):
        pass

class MyPolicy(BasePolicy, StatefulMixin):
    pass  # ç°åœ¨æœ‰äº† reset_state æ–¹æ³•
```

---

## ğŸ¯ æ–°æ¡†æ¶ä½¿ç”¨æµç¨‹

```python
# 1. åˆ›å»ºç¯å¢ƒ
from prometheus.envs import make_gym_env
env = make_gym_env("CartPole-v1")

# 2. åˆ›å»ºæ™ºèƒ½ä½“
from prometheus.agents import DQNAgent
agent = DQNAgent(state_dim=4, action_dim=2)

# 3. åˆ›å»ºè®­ç»ƒå™¨
from prometheus.trainers import DQNTrainer, TrainerConfig
config = TrainerConfig(max_episodes=500)
trainer = DQNTrainer(config)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train(env, agent)

# 5. è¯„ä¼°
metrics = trainer.evaluate(env, agent)
print(f"å¹³å‡å¾—åˆ†: {metrics['mean_score']}")
```

---

## ğŸ“Š æ¶æ„å¯¹æ¯”

### v0.0.1ï¼ˆç«ç§ç‰ˆæœ¬ï¼‰
```
æ‰€æœ‰ä»£ç åœ¨ä¸€ä¸ªæ–‡ä»¶
â””â”€â”€ core.py (Config, ReplayBuffer, QNetwork, DQNAgent)
```

### v0.1.0ï¼ˆé“¸ç‚‰ç‰ˆæœ¬ï¼‰
```
æ¨¡å—åŒ–æ¶æ„
â”œâ”€â”€ core.py (Config, ReplayBuffer)
â”œâ”€â”€ envs/ (ç¯å¢ƒæŠ½è±¡)
â”œâ”€â”€ policies/ (ç­–ç•¥æŠ½è±¡)
â”œâ”€â”€ agents/ (æ™ºèƒ½ä½“æŠ½è±¡)
â””â”€â”€ trainers/ (è®­ç»ƒå™¨æŠ½è±¡)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä»£ç æ›´æ˜“è¯»
- âœ… æ›´å®¹æ˜“æ‰©å±•
- âœ… æ›´å®¹æ˜“æµ‹è¯•
- âœ… æ›´å®¹æ˜“ç»´æŠ¤

---

## ğŸ“ ä»Šæ—¥æ€»ç»“

### å­¦åˆ°çš„è®¾è®¡åŸåˆ™ï¼š
1. **å…³æ³¨ç‚¹åˆ†ç¦»** - ä¸åŒèŒè´£æ”¾ä¸åŒæ¨¡å—
2. **ä¾èµ–å€’ç½®** - ä¾èµ–æŠ½è±¡è€Œéå…·ä½“å®ç°
3. **å¼€é—­åŸåˆ™** - å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­

### å­¦åˆ°çš„ Python æŠ€å·§ï¼š
1. **ABC (æŠ½è±¡åŸºç±»)** - å®šä¹‰æ¥å£
2. **dataclass** - ç®€åŒ–æ•°æ®ç±»
3. **Mixin** - æ··å…¥åŠŸèƒ½
4. **ç±»å‹æ³¨è§£** - æé«˜ä»£ç å¯è¯»æ€§

### æ¡†æ¶ç»“æ„ï¼š
```
prometheus/
â”œâ”€â”€ envs/       ç¯å¢ƒæŠ½è±¡
â”œâ”€â”€ policies/   ç­–ç•¥æŠ½è±¡
â”œâ”€â”€ agents/     æ™ºèƒ½ä½“æŠ½è±¡
â””â”€â”€ trainers/   è®­ç»ƒå™¨æŠ½è±¡
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

é˜¶æ®µä¸‰ï¼šæ·»æŸ´ - å®ç°æ›´å¤šç®—æ³•
- PPO (Proximal Policy Optimization)
- SAC (Soft Actor-Critic)
- æ›´å¤šç®—æ³•...

---

*ä¸‹æ¬¡æ›´æ–°ï¼šå®ç° PPO ç®—æ³•*
