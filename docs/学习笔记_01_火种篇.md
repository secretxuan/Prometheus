# Prometheus 学习笔记 #01 - 火种篇

> 第一颗火种已经点燃！

## 📖 今日学到的核心概念

### 1. 强化学习的基本要素

```
┌─────────────────────────────────────────────────────────┐
│                     强化学习循环                         │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   ┌──────┐    动作      ┌──────┐    奖励+新状态         │
│   │ 智能体 │ ────────→ │ 环境 │ ───────────────→      │
│   │ Agent │            │ Env  │                      │
│   └──────┘ ←────────  └──────┘                       │
│      ↑                         │                       │
│      └──────── 观察状态 ────────┘                       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

| 概念 | 英文 | 解释 |
|------|------|------|
| **智能体** | Agent | 做决策的一方（比如控制小车的人工智能） |
| **环境** | Environment | 被交互的世界（比如 CartPole 游戏） |
| **状态** | State | 当前情况（小车位置、速度、杆子角度...） |
| **动作** | Action | 智能体可以做的事情（左推 / 右推） |
| **奖励** | Reward | 环境给的反馈（存活一步 +1 分） |
| **Episode** | 一局 | 从开始到结束的一轮游戏 |

---

### 2. DQN 的核心思想

**Q 函数 Q(s, a)**：在状态 s 下做动作 a，之后按最优策略能获得的期望回报

```
直观理解：
Q(杆子向左倒, 向左推)  = 高分 ✓
Q(杆子向左倒, 向右推)  = 低分 ✗
```

**神经网络的作用**：
- 不是用表格存储所有 Q 值（状态太多了！）
- 而是用神经网络「学习」Q 函数
- 输入状态 → 输出每个动作的 Q 值

---

### 3. ε-贪婪策略

为什么需要随机？

```
100% 利用 → 只走已知的路，可能错过更好的
100% 探索 → 完全随机，学不到东西

ε-贪婪 → 平衡两者：
- 以 ε 的概率随机探索
- 以 1-ε 的概率选择最优动作
- ε 逐渐减小：从多探索到多利用
```

---

### 4. 经验回放

为什么需要存储经验？

```
问题：按顺序学习会有问题
- 相邻的状态太像了
- 记忆会「过拟合」到最近的经历

解决：打乱顺序，随机抽取
- 更符合「独立同分布」假设
- 一个经验可以用多次
```

---

### 5. 目标网络

为什么需要两个网络？

```
类比：打移动靶 vs 打固定靶

单网络：目标和预测都在变化，不稳定
双网络：目标网络固定一段时间，更稳定
```

---

## 🤔 常见问题解答

### Q1: 为什么我的训练结果不稳定？
A: 这是 DQN 的特点！可以尝试：
- 增大网络规模
- 调整学习率
- 使用更多训练步数

### Q2: 如何判断算法是否有效？
A: 看平均得分是否上升：
- Episode 1-50：随机水平，~20 分
- Episode 50-150：开始学习，~100 分
- Episode 150+：继续改进

### Q3: CartPole 太简单了？
A: 这是 Hello World！后面会学：
- Atari 游戏（图像输入）
- 连续动作空间
- 多智能体

---

## 📚 下一步学习计划

### 阶段一：火种（当前）✅
- [x] 运行第一个 DQN
- [ ] 理解每行代码
- [ ] 修改参数观察效果

### 阶段二：铸炉（下一步）
**目标**：设计框架基础架构

要学习的：
1. Python 抽象设计
2. 设计模式和框架架构
3. 现有框架的借鉴

### 阶段三：添柴
**目标**：实现更多算法
- PPO (目前最流行)
- SAC (连续动作)
- 更多...

### 阶段四：炼金
**目标**：性能优化
- C++ 扩展
- GPU 加速
- 并行采样

---

## 🎯 今日作业

1. **通读代码**：理解 `01_cartpole_dqn.py` 的每一行
2. **修改参数**：
   - 试试 `GAMMA = 0.9` vs `0.99`
   - 试试 `LEARNING_RATE = 0.01` vs `0.0001`
   - 观察效果变化
3. **思考问题**：
   - 如果奖励变成每步 +10，会怎样？
   - 如果动作变成 3 个（左、右、不动），需要改什么？

---

## 📖 推荐阅读

1. [Spinning Up in RL](https://spinningup.openai.com/) - OpenAI 的教程
2. [DQN 论文](https://www.nature.com/articles/nature14236) - 原始论文
3. [Gymnasium 文档](https://gymnasium.farama.org/) - 环境接口

---

*下次更新：设计 Prometheus 框架架构*
