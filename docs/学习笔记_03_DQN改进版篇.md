# Prometheus å­¦ä¹ ç¬”è®° #03 - DQN æ”¹è¿›ç‰ˆç¯‡

> ç‰ˆæœ¬ v0.2.0 - DQN æ”¹è¿›ç‰ˆï¼šDouble, Dueling, PER, Rainbow

## ğŸ“– é˜¶æ®µä¸‰ç›®æ ‡ï¼ˆDQN æ”¹è¿›ç‰ˆï¼‰

åœ¨æ ‡å‡† DQN åŸºç¡€ä¸Šï¼Œå®ç°å››ç§æ”¹è¿›ç®—æ³•ã€‚

---

## ä¸€ã€Double DQN

### æ ¸å¿ƒé—®é¢˜ï¼šQ å€¼è¿‡é«˜ä¼°è®¡

**ä¸ºä»€ä¹ˆ DQN ä¼šè¿‡é«˜ä¼°è®¡ Q å€¼ï¼Ÿ**

æ ‡å‡† DQN è®¡ç®—ç›®æ ‡ Q å€¼æ—¶ï¼š
```python
# ç›®æ ‡ç½‘ç»œæ—¢é€‰åŠ¨ä½œåˆè¯„ä¼°ä»·å€¼
next_q_values = target_network(next_states)
target_q = reward + gamma * next_q_values.max()
```

é—®é¢˜ï¼š`max` æ“ä½œä¼šé€‰åˆ°æœ€å¤§çš„ Q å€¼ï¼Œä½†è¿™ä¸ªæœ€å¤§å€¼å¯èƒ½æ˜¯**å™ªå£°**å¯¼è‡´çš„ï¼Œä¸æ˜¯çœŸå®ä»·å€¼ã€‚

### Double DQN è§£å†³æ–¹æ¡ˆ

**è§£è€¦åŠ¨ä½œé€‰æ‹©å’Œä»·å€¼è¯„ä¼°**ï¼š

```python
# ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
next_action = policy_network(next_states).argmax()

# ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°è¯¥åŠ¨ä½œçš„ä»·å€¼
next_q_values = target_network(next_states)
target_q = reward + gamma * next_q_values[next_action]
```

### é€šä¿—è§£é‡Š

| ç®—æ³• | å°±åƒ... |
|------|---------|
| æ ‡å‡† DQN | ç”¨åŒä¸€æœ¬æ•™æå‡ºé¢˜å’Œè¯„åˆ†ï¼Œå®¹æ˜“"åˆ·åˆ†" |
| Double DQN | ç”¨ä¸åŒæ•™æï¼Œå‡ºé¢˜å’Œè¯„åˆ†åˆ†å¼€ï¼Œæ›´å®¢è§‚ |

### ä»£ç å®ç°

```python
class DoubleDQNAgent(DQNAgentBase):
    def compute_target_q(self, next_states, rewards, dones):
        # Double DQN: è§£è€¦é€‰æ‹©å’Œè¯„ä¼°
        with torch.no_grad():
            # ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            next_actions = self.policy.q_network(next_states).argmax(1)
            # ç›®æ ‡ç½‘ç»œè¯„ä¼°
            next_q_target = self.target_network(next_states)
            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)
            target_q = rewards + self.config.GAMMA * next_q_values * (1 - dones)
        return target_q
```

---

## äºŒã€Dueling DQN

### æ ¸å¿ƒæ€æƒ³ï¼šåˆ†è§£ Q å€¼

æ ‡å‡† DQN ç›´æ¥å­¦ä¹  Q(s,a)ï¼Œä½†å¾ˆå¤šæƒ…å†µä¸‹ï¼š

- **çŠ¶æ€æœ¬èº«çš„å¥½å**ï¼ˆæ¯”å¦‚"å¿«åˆ°ç»ˆç‚¹äº†"ï¼‰ä¸**å…·ä½“åŠ¨ä½œ**å…³ç³»ä¸å¤§
- åªéœ€è¦è¯„ä¼°"è¿™æ˜¯ä¸ªå¥½çŠ¶æ€"ï¼Œè€Œä¸éœ€è¦ç²¾ç¡®åŒºåˆ†æ¯ä¸ªåŠ¨ä½œ

### Q å€¼åˆ†è§£å…¬å¼

```
Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
```

å…¶ä¸­ï¼š
- **V(s)**ï¼šçŠ¶æ€ä»·å€¼ï¼Œè¡¨ç¤ºè¿™ä¸ªçŠ¶æ€æœ¬èº«æœ‰å¤šå¥½
- **A(s,a)**ï¼šåŠ¨ä½œä¼˜åŠ¿ï¼Œè¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡åŠ¨ä½œå¥½å¤šå°‘

### ç½‘ç»œç»“æ„

```
        è¾“å…¥çŠ¶æ€ s
            |
        å…±äº«ç‰¹å¾å±‚
            |
      +-----+-----+
      |           |
   Value Stream  Advantage Stream
   (çŠ¶æ€ä»·å€¼ V)  (åŠ¨ä½œä¼˜åŠ¿ A)
      |           |
      +-----+-----+
            |
    Q(s,a) = V + A - mean(A)
```

### ä»£ç å®ç°

```python
class DuelingQNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim):
        self.shared_layer = nn.Sequential(...)  # å…±äº«ç‰¹å¾
        self.value_stream = nn.Sequential(...)   # V(s)
        self.advantage_stream = nn.Sequential(...)  # A(s,a)

    def forward(self, state):
        features = self.shared_layer(state)
        value = self.value_stream(features)       # [batch, 1]
        advantage = self.advantage_stream(features)  # [batch, action_dim]
        return value + advantage - advantage.mean(dim=1, keepdim=True)
```

### ä¸ºä»€ä¹ˆå‡å» mean(A)ï¼Ÿ

ä¸ºäº†ä¿è¯å¯è¯†åˆ«æ€§ï¼š
- å¦‚æœæ‰€æœ‰ A(s,a) éƒ½åŠ ä¸ŠåŒä¸€ä¸ªå¸¸æ•° cï¼ŒQ å€¼ä¸å˜
- å‡å» mean(A) å¯ä»¥å›ºå®šè¿™ä¸ªå¸¸æ•°ï¼Œä½¿å­¦ä¹ æ›´ç¨³å®š

---

## ä¸‰ã€ä¼˜å…ˆçº§ç»éªŒå›æ”¾ï¼ˆPERï¼‰

### æ ¸å¿ƒé—®é¢˜ï¼šå‡åŒ€é‡‡æ ·æ•ˆç‡ä½

æ ‡å‡† DQN ä»ç»éªŒæ± å‡åŒ€éšæœºé‡‡æ ·ï¼Œä½†ï¼š
- æœ‰äº›ç»éªŒå¾ˆ"æ™®é€š"ï¼Œå­¦ä¸å­¦å·®åˆ«ä¸å¤§
- æœ‰äº›ç»éªŒå¾ˆ"æ„å¤–"ï¼Œå€¼å¾—å¤šå­¦ä¹ å‡ æ¬¡

### è§£å†³æ–¹æ¡ˆï¼šæŒ‰ä¼˜å…ˆçº§é‡‡æ ·

**ä¼˜å…ˆçº§ = |TD è¯¯å·®| + Îµ**

TD è¯¯å·®è¶Šå¤§ â†’ é¢„æµ‹è¶Šä¸å‡† â†’ è¶Šå€¼å¾—å­¦ä¹ 

### SumTree æ•°æ®ç»“æ„

ä¸ºäº† O(log n) é‡‡æ ·ï¼Œä½¿ç”¨ SumTreeï¼š

```
            [p0+p1+p2+p3]  <- æ ¹èŠ‚ç‚¹ï¼ˆæ€»å’Œï¼‰
               /      \
         [p0+p1]      [p2+p3]
          /  \         /  \
        p0    p1     p2    p3  <- å¶å­ï¼ˆå­˜å‚¨æ•°æ®ï¼‰
```

### é‡è¦æ€§é‡‡æ ·æƒé‡

æŒ‰ä¼˜å…ˆçº§é‡‡æ ·ä¼šæ”¹å˜æ•°æ®åˆ†å¸ƒï¼Œéœ€è¦ç”¨æƒé‡ä¿®æ­£ï¼š

```python
weight = (N * P(i))^(-beta)
```

- beta ä» 0.4 çº¿æ€§å¢é•¿åˆ° 1
- è¶Šå¸¸ç”¨çš„ç»éªŒï¼Œæƒé‡è¶Šå°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰

### ä»£ç å®ç°

```python
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6, beta_start=0.4):
        self.sum_tree = SumTree(capacity)
        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.beta_start = beta_start

    def update_priorities(self, indices, td_errors):
        for idx, td_error in zip(indices, td_errors):
            priority = (abs(td_error) + epsilon) ** alpha
            self.sum_tree.update(idx, priority)
```

---

## å››ã€Rainbow DQN

### æ•´åˆæ‰€æœ‰æ”¹è¿›

Rainbow = Double + Dueling + PER

| æ”¹è¿› | è§£å†³çš„é—®é¢˜ |
|------|-----------|
| Double DQN | Q å€¼è¿‡é«˜ä¼°è®¡ |
| Dueling DQN | çŠ¶æ€ä»·å€¼ä¸åŠ¨ä½œä¼˜åŠ¿åˆ†ç¦» |
| PER | æé«˜å­¦ä¹ æ•ˆç‡ |

### ä»£ç ç»“æ„

```python
class RainbowAgent:
    def __init__(...):
        # Dueling ç½‘ç»œ
        self.policy = RainbowDQNPolicy(...)
        self.target_network = DuelingQNetwork(...)

        # PER ç¼“å†²åŒº
        self.replay_buffer = PrioritizedReplayBuffer(...)

    def learn(self):
        # PER é‡‡æ ·ï¼ˆè¿”å›ç´¢å¼•å’Œæƒé‡ï¼‰
        states, actions, rewards, next_states, dones, indices, weights = \
            self.replay_buffer.sample(batch_size)

        # Double DQN è®¡ç®—ç›®æ ‡
        next_actions = self.policy.q_network(next_states).argmax(1)
        next_q_target = self.target_network(next_states)
        next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)
        target_q = rewards + gamma * next_q_values * (1 - dones)

        # åŠ æƒæŸå¤±
        td_errors = torch.abs(target_q - q_values)
        loss = (weights * loss_fn(q_values, target_q)).mean()

        # æ›´æ–°ä¼˜å…ˆçº§
        self.replay_buffer.update_priorities(indices, td_errors)
```

---

## äº”ã€ç®—æ³•å¯¹æ¯”

| ç®—æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| DQN | ç®€å•ç¨³å®š | Q å€¼è¿‡é«˜ä¼°è®¡ | å…¥é—¨å­¦ä¹  |
| Double DQN | ä¼°å€¼æ›´å‡†ç¡® | å¢åŠ è®¡ç®—é‡ | é€šç”¨åœºæ™¯ |
| Dueling DQN | æ›´å¥½å­¦ä¹ çŠ¶æ€ä»·å€¼ | ç½‘ç»œæ›´å¤æ‚ | åŠ¨ä½œä»·å€¼ç›¸è¿‘çš„çŠ¶æ€ |
| PER | å­¦ä¹ æ•ˆç‡é«˜ | å®ç°å¤æ‚ | ç»éªŒè´¨é‡å·®å¼‚å¤§ |
| Rainbow | æ€§èƒ½æœ€å¥½ | å®ç°æœ€å¤æ‚ | è¿½æ±‚æœ€ä½³æ€§èƒ½ |

---

## å…­ã€å®éªŒç»“æœå»ºè®®

åœ¨ CartPole-v1 ä¸Šçš„é¢„æœŸè¡¨ç°ï¼š

| ç®—æ³• | å¹³å‡å¾—åˆ† | æ”¶æ•›é€Ÿåº¦ |
|------|----------|----------|
| DQN | ~400 | ä¸­ç­‰ |
| Double DQN | ~420 | ç¨å¿« |
| Dueling DQN | ~430 | ä¸­ç­‰ |
| PER | ~450 | è¾ƒå¿« |
| Rainbow | ~470+ | æœ€å¿« |

---

## ä¸ƒã€å®ç°è¦ç‚¹

### 1. å‘åå…¼å®¹æ€§

é‡æ„åä¿æŒåŸæœ‰å¯¼å…¥è·¯å¾„æœ‰æ•ˆï¼š

```python
# æ—§ä»£ç ä»ç„¶æœ‰æ•ˆ
from prometheus.agents import DQNAgent
from prometheus.policies import DQNPolicy

# æ–°ä»£ç æ¨è
from prometheus.agents.dqn import DQNAgent, DoubleDQNAgent, DuelingDQNAgent, PERAgent, RainbowAgent
```

### 2. æ¨¡å—åŒ–è®¾è®¡

```
prometheus/
â”œâ”€â”€ policies/dqn/
â”‚   â”œâ”€â”€ base.py      # DQN åŸºç±»
â”‚   â”œâ”€â”€ double.py    # Double DQN
â”‚   â”œâ”€â”€ dueling.py   # Dueling DQN
â”‚   â””â”€â”€ rainbow.py   # Rainbow
â”œâ”€â”€ agents/dqn/
â”‚   â”œâ”€â”€ base.py      # DQN åŸºç±»
â”‚   â”œâ”€â”€ double.py
â”‚   â”œâ”€â”€ dueling.py
â”‚   â”œâ”€â”€ per.py
â”‚   â””â”€â”€ rainbow.py
â””â”€â”€ core.py          # SumTree, PrioritizedReplayBuffer
```

---

## ğŸ“ ä»Šæ—¥æ€»ç»“

### å­¦åˆ°çš„çŸ¥è¯†ï¼š
1. **Double DQN**ï¼šè§£è€¦åŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼°
2. **Dueling DQN**ï¼šQ = V + A åˆ†è§£
3. **PER**ï¼šä¼˜å…ˆçº§é‡‡æ · + é‡è¦æ€§é‡‡æ ·æƒé‡
4. **SumTree**ï¼šO(log n) é‡‡æ ·å’Œæ›´æ–°
5. **Rainbow**ï¼šæ•´åˆå¤šç§æ”¹è¿›

### æ¡†æ¶å˜åŒ–ï¼š
- æ–°å¢ `policies/dqn/` å­æ¨¡å—
- æ–°å¢ `agents/dqn/` å­æ¨¡å—
- æ–°å¢ `SumTree` å’Œ `PrioritizedReplayBuffer`
- é‡æ„ DQN ä»£ç ç»“æ„

---

*ä¸‹ä¸€æ­¥ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆREINFORCE, A2C, PPOï¼‰*
