"""
=============================================
DQN è®­ç»ƒå™¨
=============================================

ä¸“é—¨è®­ç»ƒ DQN æ™ºèƒ½ä½“çš„è®­ç»ƒå™¨ã€‚
"""

import time
from pathlib import Path
from typing import Dict, Any, Optional
import numpy as np

from prometheus.trainers.base import BaseTrainer, TrainerConfig, Callback
from prometheus.agents.dqn import DQNAgent
from prometheus.envs.base import EnvWrapper


class ProgressCallback(Callback):
    """
    è¿›åº¦æ‰“å°å›è°ƒ

    æ¯éš”ä¸€å®šé—´éš”æ‰“å°è®­ç»ƒè¿›åº¦
    """

    def __init__(self, interval: int = 10):
        self.interval = interval
        self.scores = []
        self.episode_times = []

    def on_episode_end(self, trainer, episode: int, metrics: Dict):
        self.scores.append(metrics.get("score", 0))
        self.episode_times.append(metrics.get("duration", 0))

        if episode % self.interval == 0:
            avg_score = np.mean(self.scores[-self.interval:])
            eps = metrics.get("epsilon", 0)
            buffer_size = metrics.get("buffer_size", 0)

            print(f"Episode {episode:4d} | "
                  f"å¾—åˆ†: {metrics.get('score', 0):3.0f} | "
                  f"å¹³å‡: {avg_score:5.1f} | "
                  f"Îµ: {eps:.3f} | "
                  f"ç»éªŒæ± : {buffer_size}")


class DQNTrainer(BaseTrainer):
    """
    DQN è®­ç»ƒå™¨

    é€šä¿—è§£é‡Šï¼š
    -----------
    ä¸“é—¨è´Ÿè´£è®­ç»ƒ DQN æ™ºèƒ½ä½“çš„"æ•™ç»ƒ"

    ä½¿ç”¨æ–¹æ³•ï¼š
    ---------
    >>> from prometheus.envs import make_gym_env
    >>> from prometheus.agents import DQNAgent
    >>> from prometheus.trainers import DQNTrainer
    >>>
    >>> env = make_gym_env("CartPole-v1")
    >>> agent = DQNAgent(state_dim=4, action_dim=2)
    >>> trainer = DQNTrainer()
    >>> result = trainer.train(env, agent)
    """

    def __init__(self, config: TrainerConfig = None):
        super().__init__(config)
        self._setup_callbacks()

    def _setup_callbacks(self):
        """è®¾ç½®é»˜è®¤å›è°ƒ"""
        if not self.config.callbacks:
            self.config.callbacks = [ProgressCallback(interval=self.config.log_interval)]

    def train(
        self,
        env: EnvWrapper,
        agent: DQNAgent,
        target_update_interval: int = 10
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ™ºèƒ½ä½“

        Args:
            env: ç¯å¢ƒ
            agent: DQN æ™ºèƒ½ä½“
            target_update_interval: æ¯éš”å¤šå°‘ episode æ›´æ–°ç›®æ ‡ç½‘ç»œ

        Returns:
            è®­ç»ƒç»“æœ
        """
        # è·å–ç¯å¢ƒä¿¡æ¯
        state_dim = env.spec.observation_space.shape[0]
        action_dim = env.spec.action_space.shape if env.spec.action_space.shape else (env.spec.action_space,)
        if len(action_dim) == 0:
            action_dim = env.spec.action_space.shape  # è¿™å¯èƒ½éœ€è¦è°ƒæ•´
        # ç®€åŒ–å¤„ç†ï¼šå¯¹äºç¦»æ•£åŠ¨ä½œç©ºé—´
        if hasattr(env.spec.action_space._gym_space, 'n'):
            action_dim = env.spec.action_space._gym_space.n

        print("=" * 60)
        print("ğŸ›ï¸  Prometheus - DQN è®­ç»ƒ")
        print("=" * 60)
        print(f"ç¯å¢ƒ: {env.spec.name}")
        print(f"çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"åŠ¨ä½œæ•°é‡: {action_dim}")
        print(f"æœ€å¤§ Episode: {self.config.max_episodes}")
        print("=" * 60)
        print()

        # è®­ç»ƒå¾ªç¯
        for callback in self.config.callbacks:
            callback.on_train_start(self)

        for episode in range(1, self.config.max_episodes + 1):
            episode_start = time.time()

            for callback in self.config.callbacks:
                callback.on_episode_start(self, episode)

            # è¿è¡Œä¸€ä¸ª episode
            metrics = self._run_episode(env, agent)
            metrics["duration"] = time.time() - episode_start
            metrics["epsilon"] = agent._epsilon
            metrics["buffer_size"] = len(agent.replay_buffer)

            for callback in self.config.callbacks:
                callback.on_episode_end(self, episode, metrics)

            # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
            if episode % target_update_interval == 0:
                agent.update_target_network()

            # å®šæœŸè¯„ä¼°
            if episode % self.config.eval_interval == 0:
                eval_metrics = self.evaluate(env, agent, render=False)
                print(f"  â†’ è¯„ä¼°å¹³å‡å¾—åˆ†: {eval_metrics['mean_score']:.1f}")

            # å®šæœŸä¿å­˜
            if episode % self.config.save_interval == 0:
                save_path = Path(self.config.save_dir) / f"checkpoint_ep{episode}.pth"
                save_path.parent.mkdir(parents=True, exist_ok=True)
                agent.save(str(save_path))

        for callback in self.config.callbacks:
            callback.on_train_end(self)

        print()
        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)

        return {"episodes": episode, "final_score": metrics.get("score", 0)}

    def _run_episode(self, env: EnvWrapper, agent: DQNAgent) -> Dict:
        """è¿è¡Œä¸€ä¸ª episode"""
        state, _ = env.reset()
        score = 0
        done = False
        step = 0

        while not done and step < self.config.max_steps_per_episode:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)

            # å­¦ä¹ 
            if len(agent.replay_buffer) >= agent.config.BATCH_SIZE:
                agent.learn()

            state = next_state
            score += reward
            step += 1

        return {"score": score, "steps": step}

    def evaluate(
        self,
        env: EnvWrapper,
        agent: DQNAgent,
        n_episodes: int = None,
        render: bool = False
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ™ºèƒ½ä½“

        Args:
            env: ç¯å¢ƒ
            agent: æ™ºèƒ½ä½“
            n_episodes: è¯„ä¼°è½®æ•°
            render: æ˜¯å¦æ¸²æŸ“

        Returns:
            è¯„ä¼°ç»“æœ
        """
        if n_episodes is None:
            n_episodes = self.config.eval_episodes

        scores = []
        agent.set_mode(training=False)

        for _ in range(n_episodes):
            state, _ = env.reset()
            score = 0
            done = False

            while not done:
                action = agent.act(state, training=False)
                state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                score += reward

            scores.append(score)

        agent.set_mode(training=True)

        return {
            "mean_score": np.mean(scores),
            "std_score": np.std(scores),
            "min_score": np.min(scores),
            "max_score": np.max(scores)
        }

    def save_checkpoint(self, path: str, agent: DQNAgent):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        agent.save(path)

    def load_checkpoint(self, path: str, agent: DQNAgent):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        agent.load(path)
