"""
=============================================
A2C è®­ç»ƒå™¨
=============================================
"""

import time
from pathlib import Path
from typing import Dict, Any
import numpy as np

from prometheus.trainers.base import BaseTrainer, TrainerConfig, Callback
from prometheus.agents.policy_gradient.a2c import A2CAgent
from prometheus.envs.base import EnvWrapper


class ProgressCallback(Callback):
    """
    è¿›åº¦æ‰“å°å›è°ƒ
    """

    def __init__(self, interval: int = 10):
        self.interval = interval
        self.scores = []
        self.episode_times = []

    def on_episode_end(self, trainer, episode: int, metrics: Dict):
        self.scores.append(metrics.get("score", 0))
        self.episode_times.append(metrics.get("duration", 0))

        if episode % self.interval == 0:
            avg_score = np.mean(self.scores[-self.interval:])
            policy_loss = metrics.get("policy_loss", 0)
            value_loss = metrics.get("value_loss", 0)
            entropy = metrics.get("entropy", 0)

            print(f"Episode {episode:4d} | "
                  f"å¾—åˆ†: {metrics.get('score', 0):3.0f} | "
                  f"å¹³å‡: {avg_score:5.1f} | "
                  f"P_Loss: {policy_loss:.3f} | "
                  f"V_Loss: {value_loss:.3f} | "
                  f"Ent: {entropy:.3f}")


class A2CTrainer(BaseTrainer):
    """
    A2C è®­ç»ƒå™¨

    æ”¯æŒä¸¤ç§æ›´æ–°æ–¹å¼ï¼š
    1. Episode ç»“æŸåæ›´æ–°ï¼ˆé»˜è®¤ï¼‰
    2. N-step æ›´æ–°ï¼ˆæ¯ n æ­¥æ›´æ–°ä¸€æ¬¡ï¼‰
    """

    def __init__(self, config: TrainerConfig = None):
        super().__init__(config)
        self._setup_callbacks()

    def _setup_callbacks(self):
        """è®¾ç½®é»˜è®¤å›è°ƒ"""
        if not self.config.callbacks:
            self.config.callbacks = [ProgressCallback(interval=self.config.log_interval)]

    def train(
        self,
        env: EnvWrapper,
        agent: A2CAgent,
        n_step_update: bool = False
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ™ºèƒ½ä½“

        Args:
            env: ç¯å¢ƒ
            agent: A2C æ™ºèƒ½ä½“
            n_step_update: æ˜¯å¦ä½¿ç”¨ n-step æ›´æ–°

        Returns:
            è®­ç»ƒç»“æœ
        """
        # è·å–ç¯å¢ƒä¿¡æ¯
        state_dim = env.spec.observation_space.shape[0]
        action_dim = env.spec.action_space._gym_space.n

        print("=" * 60)
        print("ğŸ›ï¸  Prometheus - A2C è®­ç»ƒ")
        print("=" * 60)
        print(f"ç¯å¢ƒ: {env.spec.name}")
        print(f"çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"åŠ¨ä½œæ•°é‡: {action_dim}")
        print(f"æœ€å¤§ Episode: {self.config.max_episodes}")
        print(f"æ›´æ–°æ–¹å¼: {'N-step' if n_step_update else 'Episode-end'}")
        print("=" * 60)
        print()

        # è®­ç»ƒå¾ªç¯
        for callback in self.config.callbacks:
            callback.on_train_start(self)

        for episode in range(1, self.config.max_episodes + 1):
            episode_start = time.time()

            for callback in self.config.callbacks:
                callback.on_episode_start(self, episode)

            # è¿è¡Œä¸€ä¸ª episode
            metrics = self._run_episode(env, agent, n_step_update)
            metrics["duration"] = time.time() - episode_start

            # === å­¦ä¹ ï¼ˆepisode ç»“æŸåï¼‰===
            if not n_step_update:
                learn_metrics = agent.learn()
                metrics.update(learn_metrics)

            for callback in self.config.callbacks:
                callback.on_episode_end(self, episode, metrics)

            # å®šæœŸè¯„ä¼°
            if episode % self.config.eval_interval == 0:
                eval_metrics = self.evaluate(env, agent, render=False)
                print(f"  â†’ è¯„ä¼°å¹³å‡å¾—åˆ†: {eval_metrics['mean_score']:.1f}")

            # å®šæœŸä¿å­˜
            if episode % self.config.save_interval == 0:
                save_path = Path(self.config.save_dir) / f"a2c_ep{episode}.pth"
                save_path.parent.mkdir(parents=True, exist_ok=True)
                agent.save(str(save_path))

        for callback in self.config.callbacks:
            callback.on_train_end(self)

        print()
        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)

        return {"episodes": episode, "final_score": metrics.get("score", 0)}

    def _run_episode(self, env: EnvWrapper, agent: A2CAgent, n_step_update: bool) -> Dict:
        """è¿è¡Œä¸€ä¸ª episode"""
        agent.reset()
        state, _ = env.reset()
        score = 0
        done = False
        step = 0

        while not done and step < self.config.max_steps_per_episode:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)

            # N-step æ›´æ–°
            if n_step_update and agent.should_update():
                agent.learn()

            state = next_state
            score += reward
            step += 1

        return {"score": score, "steps": step}

    def evaluate(
        self,
        env: EnvWrapper,
        agent: A2CAgent,
        n_episodes: int = None,
        render: bool = False
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ™ºèƒ½ä½“

        Args:
            env: ç¯å¢ƒ
            agent: æ™ºèƒ½ä½“
            n_episodes: è¯„ä¼°è½®æ•°
            render: æ˜¯å¦æ¸²æŸ“

        Returns:
            è¯„ä¼°ç»“æœ
        """
        if n_episodes is None:
            n_episodes = self.config.eval_episodes

        scores = []
        agent.set_mode(training=False)

        for _ in range(n_episodes):
            agent.reset()
            state, _ = env.reset()
            score = 0
            done = False

            while not done:
                action = agent.act(state, training=False)
                state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                score += reward

            scores.append(score)

        agent.set_mode(training=True)

        return {
            "mean_score": np.mean(scores),
            "std_score": np.std(scores),
            "min_score": np.min(scores),
            "max_score": np.max(scores)
        }

    def save_checkpoint(self, path: str, agent: A2CAgent):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        agent.save(path)

    def load_checkpoint(self, path: str, agent: A2CAgent):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        agent.load(path)
