"""
=============================================
PPO è®­ç»ƒå™¨
=============================================
"""

import time
from pathlib import Path
from typing import Dict, Any
import numpy as np

from prometheus.trainers.base import BaseTrainer, TrainerConfig, Callback
from prometheus.agents.policy_gradient.ppo import PPOAgent
from prometheus.envs.base import EnvWrapper


class ProgressCallback(Callback):
    """
    è¿›åº¦æ‰“å°å›è°ƒ
    """

    def __init__(self, interval: int = 10):
        self.interval = interval
        self.scores = []
        self.episode_times = []
        self.updates = 0

    def on_episode_end(self, trainer, episode: int, metrics: Dict):
        self.scores.append(metrics.get("score", 0))
        self.episode_times.append(metrics.get("duration", 0))

        if episode % self.interval == 0:
            avg_score = np.mean(self.scores[-self.interval:])
            policy_loss = metrics.get("policy_loss", 0)
            value_loss = metrics.get("value_loss", 0)
            entropy = metrics.get("entropy", 0)

            print(f"Episode {episode:4d} | "
                  f"å¾—åˆ†: {metrics.get('score', 0):3.0f} | "
                  f"å¹³å‡: {avg_score:5.1f} | "
                  f"P_Loss: {policy_loss:.3f} | "
                  f"V_Loss: {value_loss:.3f} | "
                  f"Ent: {entropy:.3f}")

    def on_update(self, trainer, update_info: Dict):
        self.updates += 1
        if self.updates % 5 == 0:
            print(f"  â† æ›´æ–° #{self.updates} | "
                  f"æ”¶é›†æ­¥æ•°: {update_info.get('steps', 0)}")


class PPOTrainer(BaseTrainer):
    """
    PPO è®­ç»ƒå™¨

    ä¸å…¶ä»–è®­ç»ƒå™¨çš„åŒºåˆ«ï¼š
    1. æ”¶é›†ä¸€å®šæ•°é‡çš„æ­¥éª¤åæ›´æ–°ï¼Œè€Œä¸æ˜¯æ¯ä¸ª episode æ›´æ–°
    2. ä¸€æ‰¹æ•°æ®å¯ä»¥å¤šæ¬¡ä½¿ç”¨ï¼ˆmultiple epochsï¼‰
    """

    def __init__(self, config: TrainerConfig = None):
        super().__init__(config)
        self._setup_callbacks()

    def _setup_callbacks(self):
        """è®¾ç½®é»˜è®¤å›è°ƒ"""
        if not self.config.callbacks:
            self.config.callbacks = [ProgressCallback(interval=self.config.log_interval)]

    def train(
        self,
        env: EnvWrapper,
        agent: PPOAgent,
        n_epochs: int = 4,
        batch_size: int = 64
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ™ºèƒ½ä½“

        Args:
            env: ç¯å¢ƒ
            agent: PPO æ™ºèƒ½ä½“
            n_epochs: PPO æ›´æ–°è½®æ•°
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            è®­ç»ƒç»“æœ
        """
        # è·å–ç¯å¢ƒä¿¡æ¯
        state_dim = env.spec.observation_space.shape[0]
        action_dim = env.spec.action_space._gym_space.n

        print("=" * 60)
        print("ğŸ›ï¸  Prometheus - PPO è®­ç»ƒ")
        print("=" * 60)
        print(f"ç¯å¢ƒ: {env.spec.name}")
        print(f"çŠ¶æ€ç»´åº¦: {state_dim}")
        print(f"åŠ¨ä½œæ•°é‡: {action_dim}")
        print(f"æœ€å¤§ Episode: {self.config.max_episodes}")
        print(f"æ”¶é›†æ­¥æ•°: {agent.collect_steps}")
        print(f"æ›´æ–°è½®æ•°: {n_epochs}")
        print("=" * 60)
        print()

        # è®­ç»ƒå¾ªç¯
        for callback in self.config.callbacks:
            callback.on_train_start(self)

        total_steps = 0
        update_count = 0

        for episode in range(1, self.config.max_episodes + 1):
            episode_start = time.time()

            for callback in self.config.callbacks:
                callback.on_episode_start(self, episode)

            # è¿è¡Œä¸€ä¸ª episode
            metrics = self._run_episode(env, agent)
            metrics["duration"] = time.time() - episode_start
            total_steps += metrics["steps"]

            # === æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–° ===
            if agent.should_update():
                learn_metrics = agent.learn(n_epochs=n_epochs, batch_size=batch_size)
                metrics.update(learn_metrics)
                update_count += 1

                for callback in self.config.callbacks:
                    if hasattr(callback, 'on_update'):
                        callback.on_update(self, {'steps': total_steps})

                # é‡ç½®æ”¶é›†è®¡æ•°
                agent.step_count = 0

            for callback in self.config.callbacks:
                callback.on_episode_end(self, episode, metrics)

            # å®šæœŸè¯„ä¼°
            if episode % self.config.eval_interval == 0:
                eval_metrics = self.evaluate(env, agent, render=False)
                print(f"  â†’ è¯„ä¼°å¹³å‡å¾—åˆ†: {eval_metrics['mean_score']:.1f}")

            # å®šæœŸä¿å­˜
            if episode % self.config.save_interval == 0:
                save_path = Path(self.config.save_dir) / f"ppo_ep{episode}.pth"
                save_path.parent.mkdir(parents=True, exist_ok=True)
                agent.save(str(save_path))

        for callback in self.config.callbacks:
            callback.on_train_end(self)

        print()
        print("=" * 60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»æ›´æ–°æ¬¡æ•°: {update_count}")
        print("=" * 60)

        return {"episodes": episode, "final_score": metrics.get("score", 0)}

    def _run_episode(self, env: EnvWrapper, agent: PPOAgent) -> Dict:
        """è¿è¡Œä¸€ä¸ª episode"""
        state, _ = env.reset()
        score = 0
        done = False
        step = 0

        while not done and step < self.config.max_steps_per_episode:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state, training=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)

            state = next_state
            score += reward
            step += 1

        return {"score": score, "steps": step}

    def evaluate(
        self,
        env: EnvWrapper,
        agent: PPOAgent,
        n_episodes: int = None,
        render: bool = False
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ™ºèƒ½ä½“

        Args:
            env: ç¯å¢ƒ
            agent: æ™ºèƒ½ä½“
            n_episodes: è¯„ä¼°è½®æ•°
            render: æ˜¯å¦æ¸²æŸ“

        Returns:
            è¯„ä¼°ç»“æœ
        """
        if n_episodes is None:
            n_episodes = self.config.eval_episodes

        scores = []
        agent.set_mode(training=False)

        for _ in range(n_episodes):
            state, _ = env.reset()
            score = 0
            done = False

            while not done:
                action = agent.act(state, training=False)
                state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                score += reward

            scores.append(score)

        agent.set_mode(training=True)

        return {
            "mean_score": np.mean(scores),
            "std_score": np.std(scores),
            "min_score": np.min(scores),
            "max_score": np.max(scores)
        }

    def save_checkpoint(self, path: str, agent: PPOAgent):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        agent.save(path)

    def load_checkpoint(self, path: str, agent: PPOAgent):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        agent.load(path)
