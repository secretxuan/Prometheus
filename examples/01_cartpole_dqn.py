#!/usr/bin/env python3
"""
=============================================
Prometheus ç¤ºä¾‹ #01: DQN è§£å†³ CartPole
=============================================

è¿™æ˜¯å¼ºåŒ–å­¦ä¹ çš„ "Hello World"ï¼

æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ DQN (Deep Q-Network) ç®—æ³•ï¼Œ
ç”¨äºè§£å†³ CartPoleï¼ˆå°è½¦å€’ç«‹æ‘†ï¼‰ä»»åŠ¡ã€‚

å­¦ä¹ è·¯çº¿ï¼š
1. å…ˆé€šè¯»ä¸€éä»£ç ï¼Œç†è§£å¤§è‡´æµç¨‹
2. è¿è¡Œä»£ç ï¼Œçœ‹æ•ˆæœ
3. å¯¹ç…§æ³¨é‡Šï¼Œç†è§£æ¯ä¸€æ­¥
4. ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿå˜åŒ–

ä»€ä¹ˆæ˜¯ CartPoleï¼Ÿ
----------------
ä¸€ä¸ªç»å…¸çš„æ§åˆ¶ä»»åŠ¡ï¼š
- æœ‰ä¸€è¾†å°è½¦ï¼Œå¯ä»¥åœ¨æ°´å¹³è½¨é“ä¸Šå·¦å³ç§»åŠ¨
- å°è½¦ä¸Šæ–¹ç«–ç«‹ä¸€æ ¹æ†å­ï¼Œæ†å­å¯ä»¥è‡ªç”±æ‘†åŠ¨
- ç›®æ ‡ï¼šé€šè¿‡ç§»åŠ¨å°è½¦ï¼Œä¿æŒæ†å­ä¸å€’
- å¦‚æœæ†å­å€¾æ–œè¶…è¿‡ 15 åº¦ï¼Œæˆ–å°è½¦ç§»å‡ºè½¨é“ï¼Œæ¸¸æˆç»“æŸ

ä»€ä¹ˆæ˜¯ DQNï¼Ÿ
-----------
DQN (Deep Q-Network) æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å¥ åŸºæ€§ç®—æ³•ï¼Œ
ç”± DeepMind åœ¨ 2015 å¹´å‘è¡¨åœ¨ Nature ä¸Šã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
1. ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹ "åœ¨æ¯ç§çŠ¶æ€ä¸‹ï¼Œæ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼"
2. é€šè¿‡å’Œç¯å¢ƒçš„äº¤äº’ä¸æ–­æ”¹è¿›è¿™ä¸ªç½‘ç»œ
3. ä½¿ç”¨ "ç»éªŒå›æ”¾" æ¥æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡
"""

# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å…¥å¿…è¦çš„åº“
# ============================================================

import torch              # PyTorch - æ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn     # nn åŒ…å«äº†æ„å»ºç¥ç»ç½‘ç»œçš„æ¨¡å—
import torch.optim as optim  # optim åŒ…å«äº†å„ç§ä¼˜åŒ–å™¨
import numpy as np        # NumPy - æ•°å€¼è®¡ç®—åº“
import collections        # æä¾›äº† deque ç­‰æœ‰ç”¨çš„æ•°æ®ç»“æ„
import random            # éšæœºæ•°ç”Ÿæˆ
from collections import deque

# Gymnasium - å¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„æ ‡å‡†æ¥å£
# æä¾›äº†å„ç§ RL ç¯å¢ƒï¼Œä»ç®€å•çš„ CartPole åˆ°å¤æ‚çš„ Atari æ¸¸æˆ
import gymnasium as gym


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šè¶…å‚æ•°é…ç½®
# ============================================================
"""
è¶…å‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ
--------------
è¶…å‚æ•°æ˜¯ç®—æ³•ä¸­éœ€è¦äººå·¥è®¾å®šçš„å‚æ•°ï¼Œä¸æ˜¯é€šè¿‡å­¦ä¹ å¾—åˆ°çš„ã€‚
ä¸åŒçš„è¶…å‚æ•°ä¼šæ˜¾è‘—å½±å“ç®—æ³•æ€§èƒ½ã€‚

è¿™é‡Œæ¯ä¸ªå‚æ•°éƒ½æœ‰è¯¦ç»†è§£é‡Šï¼Œå»ºè®®å°è¯•ä¿®æ”¹å®ƒä»¬çœ‹çœ‹æ•ˆæœï¼
"""

class Config:
    """è¶…å‚æ•°é…ç½®"""

    # === ç¯å¢ƒç›¸å…³ ===
    ENV_NAME = "CartPole-v1"     # ç¯å¢ƒåç§°
    # v1 ç‰ˆæœ¬æœ€å¤§æ­¥æ•°æ˜¯ 500ï¼Œv0 ç‰ˆæœ¬æ˜¯ 200

    # === è®­ç»ƒç›¸å…³ ===
    EPISODES = 500              # æ€»å…±è®­ç»ƒå¤šå°‘å±€ï¼ˆä¸€è½®æ¸¸æˆå«ä¸€ä¸ª episodeï¼‰
    MAX_STEPS = 500             # æ¯å±€æœ€å¤šèµ°å¤šå°‘æ­¥

    # === DQN æ ¸å¿ƒå‚æ•° ===
    GAMMA = 0.99                # æŠ˜æ‰£å› å­ (Î³)
    """
    æŠ˜æ‰£å› å­è§£é‡Šï¼š
    - èŒƒå›´ï¼š0 åˆ° 1
    - å«ä¹‰ï¼šæœªæ¥çš„å¥–åŠ±åœ¨å½“å‰çœ‹æ¥å€¼å¤šå°‘
    - Î³ = 0.99ï¼šæ˜å¤©çš„ 1 å—é’± â‰ˆ ä»Šå¤©çš„ 0.99 å—
    - Î³ è¶Šå¤§ï¼Œè¶Šçœ‹é‡é•¿æœŸå¥–åŠ±ï¼›Î³ è¶Šå°ï¼Œè¶Šçœ‹é‡çŸ­æœŸå¥–åŠ±
    """

    LEARNING_RATE = 0.001       # å­¦ä¹ ç‡ (Î±)
    """
    å­¦ä¹ ç‡è§£é‡Šï¼š
    - èŒƒå›´ï¼š0 åˆ° 1ï¼Œé€šå¸¸å¾ˆå°å¦‚ 0.001ã€0.0001
    - å«ä¹‰ï¼šæ¯æ¬¡æ›´æ–°å‚æ•°æ—¶ï¼Œè¿ˆå¤šå¤§æ­¥å­
    - å¤ªå¤§ï¼šå¯èƒ½ä¸ç¨³å®šï¼Œæ— æ³•æ”¶æ•›
    - å¤ªå°ï¼šå­¦ä¹ å¤ªæ…¢
    """

    BATCH_SIZE = 64             # æ‰¹æ¬¡å¤§å°
    """
    æ‰¹æ¬¡å¤§å°è§£é‡Šï¼š
    - æ¯æ¬¡è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œç”¨å¤šå°‘ä¸ªæ ·æœ¬
    - å¤ªå°ï¼šè®­ç»ƒä¸ç¨³å®šï¼Œæ¢¯åº¦å™ªå£°å¤§
    - å¤ªå¤§ï¼šå†…å­˜å ç”¨å¤§ï¼Œè®­ç»ƒæ…¢
    """

    # === ç»éªŒå›æ”¾ç›¸å…³ ===
    MEMORY_SIZE = 10000         # ç»éªŒæ± å®¹é‡
    """
    ç»éªŒæ± è§£é‡Šï¼š
    - å­˜å‚¨è¿‡å»çš„ "ç»éªŒ" (çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€çŠ¶æ€)
    - è®­ç»ƒæ—¶éšæœºæŠ½å–ï¼Œæ‰“ç ´æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§
    - ç±»æ¯”ï¼šäººç±»çš„ç»éªŒæ˜¯éšæ—¶é—´ç§¯ç´¯çš„ï¼Œå›å¿†æ—¶ä¹Ÿæ˜¯éšæœºæƒ³èµ·
    """

    # === æ¢ç´¢ç›¸å…³ ===
    EPSILON_START = 1.0         # åˆå§‹æ¢ç´¢ç‡
    EPSILON_END = 0.01          # æœ€ç»ˆæ¢ç´¢ç‡
    EPSILON_DECAY = 0.995       # æ¢ç´¢ç‡è¡°å‡
    """
    Îµ-è´ªå©ªç­–ç•¥è§£é‡Šï¼š
    - Îµ (epsilon)ï¼šæ¢ç´¢çš„æ¦‚ç‡
    - Îµ = 1.0ï¼š100% éšæœºæ¢ç´¢ï¼ˆåˆšå¼€å§‹ä»€ä¹ˆéƒ½ä¸æ‡‚ï¼Œå¤šå°è¯•ï¼‰
    - Îµ = 0.01ï¼š1% éšæœºæ¢ç´¢ï¼ˆåŸºæœ¬å­¦ä¼šäº†ï¼Œå¶å°”æ¢ç´¢ï¼‰
    - æ¯æ¬¡è®­ç»ƒåï¼šÎµ = Îµ Ã— 0.995ï¼Œé€æ¸å‡å°
    """


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»éªŒå›æ”¾ç¼“å†²åŒº
# ============================================================

class ReplayBuffer:
    """
    ç»éªŒå›æ”¾ç¼“å†²åŒº

    ä»€ä¹ˆæ˜¯ç»éªŒå›æ”¾ï¼Ÿ
    ---------------
    åœ¨ä¼ ç»Ÿçš„ Q-learning ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬åªç”¨ä¸€æ¬¡å°±ä¸¢å¼ƒäº†ã€‚
    ç»éªŒå›æ”¾çš„æ€è·¯æ˜¯ï¼šæŠŠæ‰€æœ‰ç»éªŒå­˜èµ·æ¥ï¼Œè®­ç»ƒæ—¶éšæœºæŠ½å–ã€‚
    è¿™æ ·çš„å¥½å¤„ï¼š
    1. æ•°æ®åˆ©ç”¨ç‡é«˜
    2. æ‰“ç ´äº†æ—¶é—´ç›¸å…³æ€§ï¼ˆç›¸é‚»çš„çŠ¶æ€å¾ˆç›¸ä¼¼ï¼‰
    3. æ›´ç¬¦åˆ i.i.d. å‡è®¾ï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒï¼‰

    æ•°æ®ç»“æ„è§£é‡Šï¼š
    ------------
    æ¯æ¡ "ç»éªŒ" æ˜¯ä¸€ä¸ªäº”å…ƒç»„ï¼š
        (state, action, reward, next_state, done)

    - state: å½“å‰çŠ¶æ€ï¼ˆæ¯”å¦‚ï¼šå°è½¦ä½ç½®ã€é€Ÿåº¦ã€æ†å­è§’åº¦ã€è§’é€Ÿåº¦ï¼‰
    - action: é‡‡å–çš„åŠ¨ä½œï¼ˆæ¯”å¦‚ï¼šå‘å·¦æ¨ or å‘å³æ¨ï¼‰
    - reward: è·å¾—çš„å¥–åŠ±ï¼ˆæ¯”å¦‚ï¼š+1 æ¯å­˜æ´»ä¸€æ­¥ï¼‰
    - next_state: é‡‡å–åŠ¨ä½œåçš„æ–°çŠ¶æ€
    - done: æ˜¯å¦ç»“æŸï¼ˆæ¸¸æˆæ˜¯å¦ç»“æŸï¼‰
    """

    def __init__(self, capacity: int):
        """
        åˆå§‹åŒ–ç»éªŒæ± 

        Args:
            capacity: èƒ½å­˜å¤šå°‘æ¡ç»éªŒ
        """
        self.buffer = deque(maxlen=capacity)  # deque æ˜¯åŒç«¯é˜Ÿåˆ—ï¼Œè¶…å‡ºå®¹é‡è‡ªåŠ¨åˆ é™¤æ—§æ•°æ®

    def push(self, state, action, reward, next_state, done):
        """
        å­˜å…¥ä¸€æ¡ç»éªŒ

        Args:
            state: å½“å‰çŠ¶æ€ (numpy array)
            action: é‡‡å–çš„åŠ¨ä½œ (int)
            reward: è·å¾—çš„å¥–åŠ± (float)
            next_state: ä¸‹ä¸€çŠ¶æ€ (numpy array)
            done: æ˜¯å¦ç»“æŸ (bool)
        """
        # å°†ç»éªŒæ‰“åŒ…æˆ tuple å­˜å…¥
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int):
        """
        éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ

        Args:
            batch_size: è¦é‡‡æ ·çš„æ•°é‡

        Returns:
            äº”ä¸ª batch çš„ tuple
        """
        # ä» buffer ä¸­éšæœºæŠ½å– batch_size æ¡ç»éªŒ
        batch = random.sample(self.buffer, batch_size)

        # å°† batch æ‹†åˆ†æˆäº”ä¸ªç‹¬ç«‹çš„åˆ—è¡¨
        # zip(*batch) çš„ä½œç”¨ç±»ä¼¼çŸ©é˜µè½¬ç½®
        states, actions, rewards, next_states, dones = zip(*batch)

        return (
            np.array(states),       # shape: [batch_size, state_dim]
            np.array(actions),      # shape: [batch_size]
            np.array(rewards),      # shape: [batch_size]
            np.array(next_states),  # shape: [batch_size, state_dim]
            np.array(dones)         # shape: [batch_size]
        )

    def __len__(self):
        """è¿”å›å½“å‰å­˜å‚¨çš„ç»éªŒæ•°é‡"""
        return len(self.buffer)


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šQ ç½‘ç»œï¼ˆç¥ç»ç½‘ç»œï¼‰
# ============================================================

class QNetwork(nn.Module):
    """
    Q ç½‘ç»œ - DQN çš„æ ¸å¿ƒ

    ä»€ä¹ˆæ˜¯ Q ç½‘ç»œï¼Ÿ
    --------------
    Q ç½‘ç»œæ˜¯ä¸€ä¸ªå‡½æ•° approximatorï¼ˆè¿‘ä¼¼å™¨ï¼‰ï¼Œç”¨æ¥å­¦ä¹  Q å‡½æ•°ã€‚

    Q å‡½æ•° Q(s, a) çš„å«ä¹‰ï¼š
    - åœ¨çŠ¶æ€ s ä¸‹ï¼Œé‡‡å–åŠ¨ä½œ aï¼Œä¹‹åæŒ‰ç…§æœ€ä¼˜ç­–ç•¥è¡ŒåŠ¨ï¼Œèƒ½è·å¾—çš„æœŸæœ›å›æŠ¥

    ç›´è§‚ç†è§£ï¼š
    - Q(å°è½¦å‘å·¦å€¾æ–œï¼Œå‘å·¦æ¨) = å¾ˆé«˜çš„åˆ†æ•°
    - Q(å°è½¦å‘å·¦å€¾æ–œï¼Œå‘å³æ¨) = å¾ˆä½çš„åˆ†æ•°

    ä¸ºä»€ä¹ˆç”¨ç¥ç»ç½‘ç»œï¼Ÿ
    ----------------
    - çŠ¶æ€ç©ºé—´å¾ˆå¤§ï¼ˆç”šè‡³è¿ç»­ï¼‰ï¼Œæ— æ³•ç”¨è¡¨æ ¼å­˜å‚¨æ‰€æœ‰ Q å€¼
    - ç¥ç»ç½‘ç»œæ˜¯"ä¸‡èƒ½å‡½æ•°æ‹Ÿåˆå™¨"ï¼Œå¯ä»¥è¿‘ä¼¼ä»»æ„å‡½æ•°
    - çŠ¶æ€ç›¸ä¼¼æ—¶ï¼ŒQ å€¼ä¹Ÿåº”è¯¥ç›¸ä¼¼ï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰
    """

    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        """
        åˆå§‹åŒ– Q ç½‘ç»œ

        Args:
            state_dim: çŠ¶æ€çš„ç»´åº¦ï¼ˆCartPole æ˜¯ 4ï¼‰
            action_dim: åŠ¨ä½œçš„æ•°é‡ï¼ˆCartPole æ˜¯ 2ï¼šå·¦/å³ï¼‰
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(QNetwork, self).__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–

        # æ„å»ºä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œ
        # è¾“å…¥å±‚ -> éšè—å±‚1 -> éšè—å±‚2 -> è¾“å‡ºå±‚

        self.network = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šè¾“å…¥å±‚ -> éšè—å±‚
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),  # æ¿€æ´»å‡½æ•°ï¼Œå¼•å…¥éçº¿æ€§

            # ç¬¬äºŒå±‚ï¼šéšè—å±‚ -> éšè—å±‚
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),

            # è¾“å‡ºå±‚ï¼šéšè—å±‚ -> åŠ¨ä½œæ•°é‡
            nn.Linear(hidden_dim, action_dim)
            # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œç›´æ¥è¾“å‡º Q å€¼ï¼ˆå¯ä»¥æ˜¯ä»»æ„å®æ•°ï¼‰
        )

    def forward(self, state):
        """
        å‰å‘ä¼ æ’­

        Args:
            state: çŠ¶æ€å¼ é‡ï¼Œshape [batch_size, state_dim]

        Returns:
            Q å€¼ï¼Œshape [batch_size, action_dim]
            æ¯ä¸ªçŠ¶æ€å¯¹åº”æ¯ä¸ªåŠ¨ä½œçš„ Q å€¼
        """
        return self.network(state)


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šDQN æ™ºèƒ½ä½“
# ============================================================

class DQNAgent:
    """
    DQN æ™ºèƒ½ä½“

    ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ (Agent)ï¼Ÿ
    --------------------
    æ™ºèƒ½ä½“æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„"å†³ç­–è€…"ï¼Œå®ƒï¼š
    1. è§‚å¯Ÿç¯å¢ƒçŠ¶æ€
    2. åšå‡ºå†³ç­–ï¼ˆé€‰æ‹©åŠ¨ä½œï¼‰
    3. ä»ç¯å¢ƒä¸­è·å¾—å¥–åŠ±
    4. æ ¹æ®å¥–åŠ±æ›´æ–°è‡ªå·±çš„ç­–ç•¥

    è¿™é‡Œçš„ DQN æ™ºèƒ½ä½“åŒ…å«ï¼š
    - Q ç½‘ç»œï¼šé¢„æµ‹æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼
    - ç»éªŒæ± ï¼šå­˜å‚¨å’Œå›æ”¾ç»éªŒ
    - ä¼˜åŒ–å™¨ï¼šæ›´æ–°ç½‘ç»œå‚æ•°
    """

    def __init__(self, state_dim: int, action_dim: int, config: Config):
        """
        åˆå§‹åŒ– DQN æ™ºèƒ½ä½“

        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œæ•°é‡
            config: è¶…å‚æ•°é…ç½®
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config

        # åˆ›å»º Q ç½‘ç»œ
        self.q_network = QNetwork(state_dim, action_dim)

        # åˆ›å»ºç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
        # ç›®æ ‡ç½‘ç»œçš„è§£é‡Šè§ train() æ–¹æ³•
        self.target_network = QNetwork(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())
        # åˆå§‹æ—¶ï¼Œç›®æ ‡ç½‘ç»œå’Œä¸»ç½‘ç»œå®Œå…¨ä¸€æ ·

        # åˆ›å»ºä¼˜åŒ–å™¨ - Adam æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–å™¨ä¹‹ä¸€
        # å®ƒè‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡
        self.optimizer = optim.Adam(
            self.q_network.parameters(),
            lr=config.LEARNING_RATE
        )

        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(config.MEMORY_SIZE)

        # æ¢ç´¢ç‡ Îµ
        self.epsilon = config.EPSILON_START

    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰

        Îµ-è´ªå©ªç­–ç•¥ï¼š
        1. ä»¥ Îµ çš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
        2. ä»¥ 1-Îµ çš„æ¦‚ç‡é€‰æ‹© Q å€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰

        Args:
            state: å½“å‰çŠ¶æ€
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼

        Returns:
            é€‰æ‹©çš„åŠ¨ä½œï¼ˆæ•´æ•°ç´¢å¼•ï¼‰
        """
        # === æ¢ç´¢é˜¶æ®µ ===
        if training and random.random() < self.epsilon:
            # éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
            return random.randrange(self.action_dim)

        # === åˆ©ç”¨é˜¶æ®µ ===
        # å°† numpy array è½¬æ¢ä¸º PyTorch tensor
        state = torch.FloatTensor(state).unsqueeze(0)  # unsqueze(0) å¢åŠ  batch ç»´åº¦
        # shape: [state_dim] -> [1, state_dim]

        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„ Q å€¼
        with torch.no_grad():  # ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œåªæ˜¯ä¸ºäº†é¢„æµ‹
            q_values = self.q_network(state)
            # q_values.shape: [1, action_dim]

        # é€‰æ‹© Q å€¼æœ€å¤§çš„åŠ¨ä½œ
        # argmax(1) è¡¨ç¤ºåœ¨ action_dim ç»´åº¦ä¸Šå–æœ€å¤§å€¼çš„ç´¢å¼•
        action = q_values.argmax(1).item()
        # .item() å°† tensor è½¬æ¢ä¸º Python æ ‡é‡

        return action

    def store_experience(self, state, action, reward, next_state, done):
        """å­˜å‚¨ä¸€æ¡ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.replay_buffer.push(state, action, reward, next_state, done)

    def train(self):
        """
        è®­ç»ƒ Q ç½‘ç»œï¼ˆä¸€æ¬¡æ›´æ–°æ­¥éª¤ï¼‰

        DQN çš„æ ¸å¿ƒè®­ç»ƒé€»è¾‘ï¼š
        1. ä»ç»éªŒæ± ä¸­éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        2. è®¡ç®—ç›®æ ‡ Q å€¼ï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰
        3. è®¡ç®—é¢„æµ‹ Q å€¼ï¼ˆä½¿ç”¨ä¸»ç½‘ç»œï¼‰
        4. è®¡ç®—æŸå¤±ï¼Œåå‘ä¼ æ’­æ›´æ–°ä¸»ç½‘ç»œ
        5. å®šæœŸåŒæ­¥ç›®æ ‡ç½‘ç»œ

        DQN æŸå¤±å‡½æ•°ï¼š
        L = (Q(s,a) - (r + Î³ Ã— max Q'(s', a')))Â²
           â†‘              â†‘
        é¢„æµ‹å€¼         ç›®æ ‡å€¼

        Q(s,a): ä¸»ç½‘ç»œé¢„æµ‹çš„ Q å€¼
        r: å®é™…è·å¾—çš„å¥–åŠ±
        Î³: æŠ˜æ‰£å› å­
        Q'(s', a'): ç›®æ ‡ç½‘ç»œé¢„æµ‹çš„ä¸‹ä¸€çŠ¶æ€çš„ Q å€¼
        """
        # å¦‚æœç»éªŒæ± ä¸å¤Ÿï¼Œä¸è®­ç»ƒ
        if len(self.replay_buffer) < self.config.BATCH_SIZE:
            return

        # === 1. é‡‡æ ·ç»éªŒ ===
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(
            self.config.BATCH_SIZE
        )

        # è½¬æ¢ä¸º PyTorch tensor
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)  # åŠ¨ä½œæ˜¯æ•´æ•°ï¼Œç”¨ LongTensor
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)

        # === 2. è®¡ç®—ç›®æ ‡ Q å€¼ ===
        # Q_target = reward + Î³ Ã— max Q_target(next_state) Ã— (1 - done)

        # è®¡ç®—ä¸‹ä¸€çŠ¶æ€æ¯ä¸ªåŠ¨ä½œçš„ Q å€¼ï¼ˆç”¨ç›®æ ‡ç½‘ç»œï¼‰
        with torch.no_grad():  # ç›®æ ‡ç½‘ç»œä¸éœ€è¦æ¢¯åº¦
            next_q_values = self.target_network(next_states)
            # shape: [batch_size, action_dim]

            # å–æœ€å¤§å€¼
            next_q_max = next_q_values.max(1)[0]
            # max(1)[0] è¡¨ç¤ºåœ¨ç¬¬ 1 ç»´åº¦ä¸Šå–æœ€å¤§å€¼ï¼Œå¹¶è¿”å›å€¼è€Œéç´¢å¼•
            # shape: [batch_size]

            # è®¡ç®—ç›®æ ‡ Q å€¼
            # done=1 æ—¶è¡¨ç¤ºæ¸¸æˆç»“æŸï¼Œæ²¡æœ‰æœªæ¥å¥–åŠ±
            # done=0 æ—¶è¡¨ç¤ºç»§ç»­ï¼Œæœ‰æœªæ¥å¥–åŠ±
            target_q_values = rewards + self.config.GAMMA * next_q_max * (1 - dones)
            # shape: [batch_size]

        # === 3. è®¡ç®—å½“å‰ Q å€¼ ===
        # æˆ‘ä»¬éœ€è¦çš„æ˜¯ "åœ¨çŠ¶æ€ s ä¸‹ï¼Œé€‰æ‹©åŠ¨ä½œ a çš„ Q å€¼"
        # gather æ“ä½œç”¨äºä»å¤šä¸ª Q å€¼ä¸­å–å‡ºç‰¹å®šåŠ¨ä½œçš„ Q å€¼

        # å…ˆè®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„ Q å€¼
        current_q_values = self.q_network(states)
        # shape: [batch_size, action_dim]

        # å–å‡ºå®é™…é‡‡å–çš„åŠ¨ä½œçš„ Q å€¼
        # actions.unsqueeze(1) å°† [batch_size] å˜æˆ [batch_size, 1]
        # gather(1, actions.unsqueeze(1)) æ²¿ action_dim ç»´åº¦æ”¶é›†
        action_indices = actions.unsqueeze(1)
        q_values = current_q_values.gather(1, action_indices).squeeze(1)
        # shape: [batch_size]

        # === 4. è®¡ç®—æŸå¤±å¹¶æ›´æ–° ===
        # ä½¿ç”¨ Huber Lossï¼ˆSmooth L1 Lossï¼‰ï¼Œæ¯” MSE æ›´é²æ£’
        loss = nn.SmoothL1Loss()(q_values, target_q_values)

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()  # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
        loss.backward()             # è®¡ç®—æ¢¯åº¦
        self.optimizer.step()       # æ›´æ–°å‚æ•°

        # === 5. æ›´æ–°æ¢ç´¢ç‡ ===
        self.epsilon = max(
            self.config.EPSILON_END,
            self.epsilon * self.config.EPSILON_DECAY
        )

        return loss.item()

    def update_target_network(self):
        """
        å°†ä¸»ç½‘ç»œçš„å‚æ•°å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ

        ä¸ºä»€ä¹ˆè¦ç”¨ç›®æ ‡ç½‘ç»œï¼Ÿ
        ------------------
        è¿™æ˜¯ä¸€ä¸ªè®­ç»ƒç¨³å®šæ€§çš„æŠ€å·§ã€‚

        é—®é¢˜ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªç½‘ç»œï¼Œç›®æ ‡å€¼å’Œé¢„æµ‹å€¼éƒ½æ¥è‡ªåŒä¸€ä¸ªç½‘ç»œï¼Œ
        è®­ç»ƒä¼šä¸ç¨³å®šï¼ˆ"è¿½é€ç§»åŠ¨çš„ç›®æ ‡"ï¼‰ã€‚

        è§£å†³ï¼šä½¿ç”¨ä¸€ä¸ª"å†»ç»“"çš„ç›®æ ‡ç½‘ç»œï¼Œæ¯éš”ä¸€æ®µæ—¶é—´æ‰æ›´æ–°ã€‚
        è¿™æ ·ç›®æ ‡å€¼åœ¨ä¸€æ®µæ—¶é—´å†…æ˜¯å›ºå®šçš„ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚

        ç±»æ¯”ï¼šæ‰“é¶æ—¶ï¼Œé¶å­ä¸åŠ¨æ¯”é¶å­ä¸€ç›´ç§»åŠ¨æ›´å®¹æ˜“ç„å‡†ã€‚
        """
        self.target_network.load_state_dict(self.q_network.state_dict())


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šè®­ç»ƒå¾ªç¯
# ============================================================

def train():
    """
    ä¸»è®­ç»ƒå¾ªç¯

    è®­ç»ƒæµç¨‹ï¼š
    1. åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    2. å¯¹æ¯ä¸ª episodeï¼š
       a. é‡ç½®ç¯å¢ƒ
       b. å¯¹æ¯ä¸ªæ­¥éª¤ï¼š
          - é€‰æ‹©åŠ¨ä½œ
          - æ‰§è¡ŒåŠ¨ä½œï¼Œè·å¾—åé¦ˆ
          - å­˜å‚¨ç»éªŒ
          - è®­ç»ƒç½‘ç»œ
    3. å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
    """

    # === åˆ›å»ºç¯å¢ƒ ===
    env = gym.make(Config.ENV_NAME, render_mode=None)  # ä¸æ¸²æŸ“ï¼Œè®­ç»ƒæ›´å¿«
    # render_mode="human" å¯ä»¥çœ‹åˆ°åŠ¨ç”»ï¼Œä½†ä¼šæ…¢å¾ˆå¤š

    # è·å–ç¯å¢ƒä¿¡æ¯
    state_dim = env.observation_space.shape[0]   # CartPole: 4
    action_dim = env.action_space.n              # CartPole: 2

    print(f"=== ç¯å¢ƒä¿¡æ¯ ===")
    print(f"ç¯å¢ƒåç§°: {Config.ENV_NAME}")
    print(f"çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"åŠ¨ä½œæ•°é‡: {action_dim}")
    print(f"çŠ¶æ€å«ä¹‰: å°è½¦ä½ç½®ã€å°è½¦é€Ÿåº¦ã€æ†å­è§’åº¦ã€æ†å­è§’é€Ÿåº¦")
    print(f"åŠ¨ä½œå«ä¹‰: 0=å‘å·¦æ¨, 1=å‘å³æ¨")
    print()

    # === åˆ›å»ºæ™ºèƒ½ä½“ ===
    agent = DQNAgent(state_dim, action_dim, Config)

    # === è®­ç»ƒå¾ªç¯ ===
    scores = []  # è®°å½•æ¯ä¸ª episode çš„å¾—åˆ†
    avg_scores = []  # è®°å½•å¹³å‡å¾—åˆ†

    print("=== å¼€å§‹è®­ç»ƒ ===")
    print(f"æ€» Episode æ•°: {Config.EPISODES}")
    print()

    for episode in range(1, Config.EPISODES + 1):
        # é‡ç½®ç¯å¢ƒ
        state, _ = env.reset()  # state æ˜¯åˆå§‹çŠ¶æ€
        score = 0  # æœ¬ episode çš„æ€»å¥–åŠ±
        loss = None

        # === ä¸€ä¸ª episode ===
        for step in range(Config.MAX_STEPS):
            # 1. é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(state, training=True)

            # 2. æ‰§è¡ŒåŠ¨ä½œ
            # env.step() è¿”å›:
            # - next_state: æ–°çŠ¶æ€
            # - reward: å¥–åŠ±
            # - terminated: æ˜¯å¦æˆåŠŸç»“æŸï¼ˆå¦‚æ†å­å€’äº†ï¼‰
            # - truncated: æ˜¯å¦è¢«æˆªæ–­ï¼ˆå¦‚è¶…è¿‡æœ€å¤§æ­¥æ•°ï¼‰
            # - info: é¢å¤–ä¿¡æ¯
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # 3. å­˜å‚¨ç»éªŒ
            agent.store_experience(state, action, reward, next_state, done)

            # 4. è®­ç»ƒï¼ˆå¦‚æœç»éªŒæ± è¶³å¤Ÿï¼‰
            if len(agent.replay_buffer) >= Config.BATCH_SIZE:
                loss = agent.train()

            # 5. æ›´æ–°çŠ¶æ€
            state = next_state
            score += reward

            # 6. å¦‚æœç»“æŸï¼Œè·³å‡ºå¾ªç¯
            if done:
                break

        # === Episode ç»“æŸåçš„å¤„ç† ===
        scores.append(score)

        # è®¡ç®—æœ€è¿‘ 10 ä¸ª episode çš„å¹³å‡å¾—åˆ†
        if len(scores) >= 10:
            avg_score = sum(scores[-10:]) / 10
            avg_scores.append(avg_score)
        else:
            avg_scores.append(sum(scores) / len(scores))

        # æ¯ 10 ä¸ª episode æ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
        if episode % 10 == 0:
            agent.update_target_network()

        # === æ‰“å°è¿›åº¦ ===
        if episode % 10 == 0:
            print(f"Episode {int(episode):3d} | "
                  f"å¾—åˆ†: {int(score):3d} | "
                  f"å¹³å‡å¾—åˆ†: {avg_scores[-1]:5.1f} | "
                  f"Îµ: {agent.epsilon:.3f} | "
                  f"ç»éªŒæ± : {len(agent.replay_buffer)}")

        # === æ£€æŸ¥æ˜¯å¦æˆåŠŸ ===
        # CartPole-v1 çš„æˆåŠŸæ ‡å‡†æ˜¯å¹³å‡å¾—åˆ† >= 475
        if len(avg_scores) >= 10 and avg_scores[-1] >= 475:
            print(f"\nğŸ‰ æ­å–œï¼åœ¨ç¬¬ {episode} ä¸ª episode è¾¾åˆ°æˆåŠŸæ ‡å‡†ï¼")
            print(f"   å¹³å‡å¾—åˆ†: {avg_scores[-1]:.1f} >= 475")
            break

    env.close()

    return scores, avg_scores


# ============================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå¯è§†åŒ–ç»“æœ
# ============================================================

def plot_results(scores, avg_scores):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿

    Args:
        scores: æ¯ä¸ª episode çš„å¾—åˆ†
        avg_scores: æ¯ä¸ª episode çš„å¹³å‡å¾—åˆ†
    """
    try:
        import matplotlib.pyplot as plt

        plt.figure(figsize=(12, 5))

        # å­å›¾1: å¾—åˆ†æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(scores, alpha=0.6, label='å•æ¬¡å¾—åˆ†')
        plt.plot(avg_scores, linewidth=2, label='å¹³å‡å¾—åˆ†ï¼ˆ10epï¼‰')
        plt.axhline(y=475, color='r', linestyle='--', label='æˆåŠŸçº¿ (475)')
        plt.xlabel('Episode')
        plt.ylabel('å¾—åˆ†')
        plt.title('è®­ç»ƒè¿›åº¦')
        plt.legend()
        plt.grid(alpha=0.3)

        # å­å›¾2: æœ€ç»ˆå¾—åˆ†åˆ†å¸ƒ
        plt.subplot(1, 2, 2)
        if len(scores) >= 50:
            recent_scores = scores[-50:]
        else:
            recent_scores = scores
        plt.hist(recent_scores, bins=20, edgecolor='black')
        plt.xlabel('å¾—åˆ†')
        plt.ylabel('æ¬¡æ•°')
        plt.title('å¾—åˆ†åˆ†å¸ƒï¼ˆæœ€è¿‘ï¼‰')
        plt.axvline(x=475, color='r', linestyle='--', label='æˆåŠŸçº¿')
        plt.grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig('examples/training_results.png', dpi=100)
        print("\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: examples/training_results.png")
    except Exception as e:
        print(f"\nâš ï¸  ç»˜å›¾å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿å®‰è£…äº† matplotlib: pip install matplotlib")


# ============================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ============================================================

def main():
    """
    ä¸»å‡½æ•° - ç¨‹åºå…¥å£
    """
    print("=" * 60)
    print("ğŸ›ï¸  Prometheus - DQN è®­ç»ƒç¤ºä¾‹")
    print("=" * 60)
    print()

    # è®­ç»ƒ
    scores, avg_scores = train()

    print()
    print("=" * 60)
    print("ğŸ“Š è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ç»ˆå¹³å‡å¾—åˆ†: {avg_scores[-1]:.1f}")
    print(f"   æœ€é«˜å¾—åˆ†: {max(scores)}")
    print("=" * 60)

    # ç»˜å›¾
    plot_results(scores, avg_scores)


if __name__ == "__main__":
    main()
