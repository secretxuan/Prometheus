"""
=============================================
PPOï¼ˆProximal Policy Optimizationï¼‰ç¤ºä¾‹
=============================================

PPO æ˜¯ç›®å‰æœ€æµè¡Œã€æœ€å®ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ã€‚

æ ¸å¿ƒç‰¹ç‚¹ï¼š
1. ä½¿ç”¨ Clipped Surrogate Objective é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
2. ä¸€æ‰¹æ•°æ®å¯ä»¥å¤šæ¬¡ä½¿ç”¨ï¼ˆé«˜æ•ˆï¼‰
3. ç®€å•ã€ç¨³å®šã€é«˜æ•ˆ

PPO vs å…¶ä»–ç®—æ³•ï¼š
- REINFORCEï¼šé«˜æ–¹å·®ï¼Œéœ€è¦å®Œæ•´ episode
- A2Cï¼šæ–¹å·®æ›´ä½ï¼Œä½†å¯èƒ½ä¸ç¨³å®š
- PPOï¼šç¨³å®šä¸”é«˜æ•ˆï¼Œæ˜¯å¾ˆå¤šé¡¹ç›®çš„é¦–é€‰

é€‚ç”¨åœºæ™¯ï¼š
- è¿ç»­åŠ¨ä½œç©ºé—´å’Œç¦»æ•£åŠ¨ä½œç©ºé—´éƒ½é€‚ç”¨
- éœ€è¦ç¨³å®šè®­ç»ƒçš„åœºæ™¯
- éœ€è¦æ ·æœ¬æ•ˆç‡çš„åœºæ™¯
"""

import torch
from prometheus.envs.gym_wrapper import make_gym_env
from prometheus.agents.policy_gradient.ppo import PPOAgent
from prometheus.trainers.policy_gradient.ppo import PPOTrainer, TrainerConfig
from prometheus.core import Config


def main():
    # === åˆ›å»ºç¯å¢ƒ ===
    env = make_gym_env("CartPole-v1")

    # === é…ç½® ===
    # è®¾ç½®å…¨å±€ Configï¼ˆé™æ€ç±»ï¼Œç›´æ¥ä¿®æ”¹å±æ€§ï¼‰
    Config.LEARNING_RATE = 3e-4     # PPO é€šå¸¸ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
    Config.GAMMA = 0.99

    trainer_config = TrainerConfig(
        max_episodes=3000,
        max_steps_per_episode=500,
        eval_interval=50,
        eval_episodes=10,
        save_interval=150,
        save_dir="checkpoints/ppo",
        log_interval=10,
    )

    # === åˆ›å»ºæ™ºèƒ½ä½“ ===
    state_dim = env.spec.observation_space.shape[0]
    action_dim = env.spec.action_space._gym_space.n

    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # === åˆ›å»ºè®­ç»ƒå™¨ ===
    trainer = PPOTrainer(config=trainer_config)

    # === å¼€å§‹è®­ç»ƒ ===
    # PPO å‚æ•°
    n_epochs = 4          # æ¯æ¬¡æ”¶é›†æ•°æ®åæ›´æ–°å¤šå°‘è½®
    batch_size = 64       # æ‰¹é‡å¤§å°

    trainer.train(env, agent, n_epochs=n_epochs, batch_size=batch_size)

    # === æœ€ç»ˆè¯„ä¼° ===
    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°")
    print("=" * 60)
    final_metrics = trainer.evaluate(env, agent, n_episodes=20)
    for k, v in final_metrics.items():
        print(f"  {k}: {v:.2f}")

    env.close()


if __name__ == "__main__":
    main()
