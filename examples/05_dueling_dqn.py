"""
=============================================
Dueling DQN ç¤ºä¾‹
=============================================

Dueling DQN å°† Q å€¼åˆ†è§£ä¸ºçŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿ï¼š
    Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from prometheus.envs import make_gym_env
from prometheus.agents.dqn import DQNAgent, DuelingDQNAgent
from prometheus.trainers import DQNTrainer, TrainerConfig


def analyze_dueling_network():
    """åˆ†æ Dueling ç½‘ç»œçš„è¾“å‡º"""
    print("=" * 60)
    print("ğŸ”¬ Dueling DQN ç½‘ç»œç»“æ„åˆ†æ")
    print("=" * 60)

    from prometheus.agents.dqn import DuelingDQNAgent
    from prometheus.core import Config

    agent = DuelingDQNAgent(state_dim=4, action_dim=2)

    # æ¨¡æ‹Ÿä¸€ä¸ªçŠ¶æ€
    state = np.array([0, 0, 0, 0], dtype=np.float32)

    # è·å– Q å€¼
    q_values = agent.policy.get_q_values(state)
    print(f"\nQ å€¼: {q_values}")

    # åˆ†åˆ«è·å– V(s) å’Œ A(s,a)
    value, advantage = agent.policy.get_value_and_advantage(state)
    print(f"\nçŠ¶æ€ä»·å€¼ V(s): {value:.4f}")
    print(f"åŠ¨ä½œä¼˜åŠ¿ A(s,a): {advantage}")

    # éªŒè¯ Q = V + A - mean(A)
    q_computed = value + advantage - advantage.mean()
    print(f"\néªŒè¯ Q(s,a) = V(s) + A(s,a) - mean(A): {q_computed}")
    print(f"ç›´æ¥ Q(s,a):                     {q_values}")
    print(f"åŒ¹é…: {np.allclose(q_values, q_computed)}")


def train_dueling_dqn():
    """è®­ç»ƒ Dueling DQN"""
    print("\n" + "=" * 60)
    print("ğŸ“Š è®­ç»ƒ Dueling DQN")
    print("=" * 60)

    env = make_gym_env("CartPole-v1")
    agent = DuelingDQNAgent(state_dim=4, action_dim=2)
    config = TrainerConfig(max_episodes=300, eval_interval=50, log_interval=20)
    trainer = DQNTrainer(config)

    result = trainer.train(env, agent)
    print(f"\nDueling DQN æœ€ç»ˆå¾—åˆ†: {result['final_score']:.1f}")
    return agent, result


def compare_with_standard():
    """å¯¹æ¯”æ ‡å‡† DQN å’Œ Dueling DQN"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ æ ‡å‡† DQN vs Dueling DQN")
    print("=" * 60)

    config = TrainerConfig(
        max_episodes=200,
        eval_interval=100,
        log_interval=50,
        save_interval=1000
    )

    # è®­ç»ƒæ ‡å‡† DQN
    print("\n--- æ ‡å‡† DQN ---")
    env = make_gym_env("CartPole-v1")
    agent_standard = DQNAgent(state_dim=4, action_dim=2)
    trainer_standard = DQNTrainer(config)
    trainer_standard.train(env, agent_standard)
    eval_standard = trainer_standard.evaluate(env, agent_standard, n_episodes=20)
    print(f"\næ ‡å‡† DQN è¯„ä¼°å¾—åˆ†: {eval_standard['mean_score']:.1f} Â± {eval_standard['std_score']:.1f}")

    # è®­ç»ƒ Dueling DQN
    print("\n--- Dueling DQN ---")
    env = make_gym_env("CartPole-v1")
    agent_dueling = DuelingDQNAgent(state_dim=4, action_dim=2)
    trainer_dueling = DQNTrainer(config)
    trainer_dueling.train(env, agent_dueling)
    eval_dueling = trainer_dueling.evaluate(env, agent_dueling, n_episodes=20)
    print(f"\nDueling DQN è¯„ä¼°å¾—åˆ†: {eval_dueling['mean_score']:.1f} Â± {eval_dueling['std_score']:.1f}")

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    print(f"æ ‡å‡† DQN:      {eval_standard['mean_score']:.1f}")
    print(f"Dueling DQN:   {eval_dueling['mean_score']:.1f}")
    improvement = eval_dueling['mean_score'] - eval_standard['mean_score']
    print(f"æå‡:          {improvement:+.1f}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Dueling DQN ç¤ºä¾‹")
    parser.add_argument("--mode", choices=["analyze", "train", "compare"],
                        default="compare", help="è¿è¡Œæ¨¡å¼")
    args = parser.parse_args()

    if args.mode == "analyze":
        analyze_dueling_network()
    elif args.mode == "train":
        train_dueling_dqn()
    else:
        compare_with_standard()
